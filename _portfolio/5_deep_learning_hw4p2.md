---
title: "Attention-based End-to-End Speech-to-Text Deep Neural Network"
excerpt: "<img src='/images/deep_learning_hw4p2/network_architecture.png' height = '200' width='200'>"
collection: portfolio
---

<img src="/images/deep_learning_hw4p2/network_architecture.png">

* Implemented character-based encoder, decoder, and attention modules for non order synchronous seq to seq translation based on  "Listen, Attend and Spell" [Chan et al.](https://arxiv.org/abs/1508.01211 "paper") 
* Implemented pyramidal bi-LSTM layers as part of the encoder module.
* Implemented random search over the output of the decoder to select the most probable sequence.


[Kaggle](https://www.kaggle.com/siddharthghiya/competitions "Kaggle")

